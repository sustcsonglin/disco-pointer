

optimizer:
  #  Adam-oriented deep learning
  _target_: torch.optim.Adam
  #  These are all default parameters for the Adam optimizer
  lr: 2e-3
  betas: [ 0.9, 0.9]
  eps: 1e-12
  weight_decay: 0

use_lr_scheduler: True
lr_scheduler_name: exponential
only_embeder: False

lr_scheduler:
  _target_: torch.optim.lr_scheduler.ExponentialLR
  interval: 'step'
  frequency: 1
  gamma: 1

